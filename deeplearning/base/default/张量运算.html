<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>张量运算 | 刘新冬的博客</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/blog/img/logo.png">
    <meta name="description" content="张量运算">
    
    <link rel="preload" href="/blog/assets/css/0.styles.495eacbe.css" as="style"><link rel="preload" href="/blog/assets/js/app.ba74f136.js" as="script"><link rel="preload" href="/blog/assets/js/2.ba6b1436.js" as="script"><link rel="preload" href="/blog/assets/js/1.2d4ce02f.js" as="script"><link rel="preload" href="/blog/assets/js/32.e50cc8ec.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.14082ad8.js"><link rel="prefetch" href="/blog/assets/js/100.dcafd219.js"><link rel="prefetch" href="/blog/assets/js/101.74760706.js"><link rel="prefetch" href="/blog/assets/js/102.cbf39534.js"><link rel="prefetch" href="/blog/assets/js/103.a7597fa2.js"><link rel="prefetch" href="/blog/assets/js/104.24531299.js"><link rel="prefetch" href="/blog/assets/js/105.10b2a4b3.js"><link rel="prefetch" href="/blog/assets/js/106.3ff91c4f.js"><link rel="prefetch" href="/blog/assets/js/107.ffabd889.js"><link rel="prefetch" href="/blog/assets/js/108.35f542ca.js"><link rel="prefetch" href="/blog/assets/js/109.bd8f74e0.js"><link rel="prefetch" href="/blog/assets/js/11.d36ef131.js"><link rel="prefetch" href="/blog/assets/js/110.182a6fc4.js"><link rel="prefetch" href="/blog/assets/js/111.947ac851.js"><link rel="prefetch" href="/blog/assets/js/112.08456d8c.js"><link rel="prefetch" href="/blog/assets/js/113.585e06ab.js"><link rel="prefetch" href="/blog/assets/js/114.86dcb468.js"><link rel="prefetch" href="/blog/assets/js/115.7b77e604.js"><link rel="prefetch" href="/blog/assets/js/116.79e3c86b.js"><link rel="prefetch" href="/blog/assets/js/117.caf49482.js"><link rel="prefetch" href="/blog/assets/js/118.0b77e591.js"><link rel="prefetch" href="/blog/assets/js/119.ce29e687.js"><link rel="prefetch" href="/blog/assets/js/12.c5493e01.js"><link rel="prefetch" href="/blog/assets/js/120.c90b1f67.js"><link rel="prefetch" href="/blog/assets/js/121.48f52b13.js"><link rel="prefetch" href="/blog/assets/js/122.2af9cf66.js"><link rel="prefetch" href="/blog/assets/js/13.430eca0c.js"><link rel="prefetch" href="/blog/assets/js/14.c88f86cd.js"><link rel="prefetch" href="/blog/assets/js/15.e1cc950a.js"><link rel="prefetch" href="/blog/assets/js/16.b699b95c.js"><link rel="prefetch" href="/blog/assets/js/17.b99ba729.js"><link rel="prefetch" href="/blog/assets/js/18.7dff4b06.js"><link rel="prefetch" href="/blog/assets/js/19.ff06d982.js"><link rel="prefetch" href="/blog/assets/js/20.7de9995c.js"><link rel="prefetch" href="/blog/assets/js/21.cb9bdcf0.js"><link rel="prefetch" href="/blog/assets/js/22.16dfd536.js"><link rel="prefetch" href="/blog/assets/js/23.eb560dbc.js"><link rel="prefetch" href="/blog/assets/js/24.661e2539.js"><link rel="prefetch" href="/blog/assets/js/25.c7f21645.js"><link rel="prefetch" href="/blog/assets/js/26.ed9b5095.js"><link rel="prefetch" href="/blog/assets/js/27.e74d660a.js"><link rel="prefetch" href="/blog/assets/js/28.f119234c.js"><link rel="prefetch" href="/blog/assets/js/29.4ebed3b0.js"><link rel="prefetch" href="/blog/assets/js/3.00356991.js"><link rel="prefetch" href="/blog/assets/js/30.64faf300.js"><link rel="prefetch" href="/blog/assets/js/31.34e09941.js"><link rel="prefetch" href="/blog/assets/js/33.9461dbb1.js"><link rel="prefetch" href="/blog/assets/js/34.e4da7325.js"><link rel="prefetch" href="/blog/assets/js/35.2045a65d.js"><link rel="prefetch" href="/blog/assets/js/36.3a5bd736.js"><link rel="prefetch" href="/blog/assets/js/37.7f16fe49.js"><link rel="prefetch" href="/blog/assets/js/38.b943265e.js"><link rel="prefetch" href="/blog/assets/js/39.998540b3.js"><link rel="prefetch" href="/blog/assets/js/4.7fa5c39e.js"><link rel="prefetch" href="/blog/assets/js/40.6aff5b27.js"><link rel="prefetch" href="/blog/assets/js/41.260aa7d0.js"><link rel="prefetch" href="/blog/assets/js/42.feaf6bdd.js"><link rel="prefetch" href="/blog/assets/js/43.53183817.js"><link rel="prefetch" href="/blog/assets/js/44.6dab230d.js"><link rel="prefetch" href="/blog/assets/js/45.b6fd5fa7.js"><link rel="prefetch" href="/blog/assets/js/46.4d50f591.js"><link rel="prefetch" href="/blog/assets/js/47.a769691a.js"><link rel="prefetch" href="/blog/assets/js/48.8a2599f2.js"><link rel="prefetch" href="/blog/assets/js/49.1c846391.js"><link rel="prefetch" href="/blog/assets/js/5.8a935be6.js"><link rel="prefetch" href="/blog/assets/js/50.a61291b3.js"><link rel="prefetch" href="/blog/assets/js/51.7dceb146.js"><link rel="prefetch" href="/blog/assets/js/52.441d5a51.js"><link rel="prefetch" href="/blog/assets/js/53.046b88a2.js"><link rel="prefetch" href="/blog/assets/js/54.b5596254.js"><link rel="prefetch" href="/blog/assets/js/55.f9077d07.js"><link rel="prefetch" href="/blog/assets/js/56.7cd32adc.js"><link rel="prefetch" href="/blog/assets/js/57.07a79e6c.js"><link rel="prefetch" href="/blog/assets/js/58.3c2714d1.js"><link rel="prefetch" href="/blog/assets/js/59.2942e8a9.js"><link rel="prefetch" href="/blog/assets/js/6.c6745fa4.js"><link rel="prefetch" href="/blog/assets/js/60.2034c67b.js"><link rel="prefetch" href="/blog/assets/js/61.af320179.js"><link rel="prefetch" href="/blog/assets/js/62.fec720d6.js"><link rel="prefetch" href="/blog/assets/js/63.34fd5f71.js"><link rel="prefetch" href="/blog/assets/js/64.08909954.js"><link rel="prefetch" href="/blog/assets/js/65.c5c5f29e.js"><link rel="prefetch" href="/blog/assets/js/66.1f2fccf4.js"><link rel="prefetch" href="/blog/assets/js/67.4fe4556f.js"><link rel="prefetch" href="/blog/assets/js/68.b4beef25.js"><link rel="prefetch" href="/blog/assets/js/69.c6a9d2c9.js"><link rel="prefetch" href="/blog/assets/js/7.f127ebf8.js"><link rel="prefetch" href="/blog/assets/js/70.00b93126.js"><link rel="prefetch" href="/blog/assets/js/71.133e5d42.js"><link rel="prefetch" href="/blog/assets/js/72.99877bb3.js"><link rel="prefetch" href="/blog/assets/js/73.957556a6.js"><link rel="prefetch" href="/blog/assets/js/74.e4a072d4.js"><link rel="prefetch" href="/blog/assets/js/75.0cd14136.js"><link rel="prefetch" href="/blog/assets/js/76.bd566a9e.js"><link rel="prefetch" href="/blog/assets/js/77.c6f42303.js"><link rel="prefetch" href="/blog/assets/js/78.e9ebefa1.js"><link rel="prefetch" href="/blog/assets/js/79.72d36919.js"><link rel="prefetch" href="/blog/assets/js/80.a61bc76f.js"><link rel="prefetch" href="/blog/assets/js/81.a082ccbf.js"><link rel="prefetch" href="/blog/assets/js/82.ef0d1455.js"><link rel="prefetch" href="/blog/assets/js/83.59d4128f.js"><link rel="prefetch" href="/blog/assets/js/84.5b2501ff.js"><link rel="prefetch" href="/blog/assets/js/85.cfb6864a.js"><link rel="prefetch" href="/blog/assets/js/86.5487c200.js"><link rel="prefetch" href="/blog/assets/js/87.27164930.js"><link rel="prefetch" href="/blog/assets/js/88.1f0b6531.js"><link rel="prefetch" href="/blog/assets/js/89.0fe143c2.js"><link rel="prefetch" href="/blog/assets/js/90.e658e2b0.js"><link rel="prefetch" href="/blog/assets/js/91.f53450dd.js"><link rel="prefetch" href="/blog/assets/js/92.c604ad6a.js"><link rel="prefetch" href="/blog/assets/js/93.bfae73e5.js"><link rel="prefetch" href="/blog/assets/js/94.87427a06.js"><link rel="prefetch" href="/blog/assets/js/95.2a709d19.js"><link rel="prefetch" href="/blog/assets/js/96.2d7b8fe3.js"><link rel="prefetch" href="/blog/assets/js/97.e9923d97.js"><link rel="prefetch" href="/blog/assets/js/98.fd7ee458.js"><link rel="prefetch" href="/blog/assets/js/99.6a29e68b.js"><link rel="prefetch" href="/blog/assets/js/vendors~docsearch.162bdabc.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.495eacbe.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/img/logo.png" alt="刘新冬的博客" class="logo"> <span class="site-name can-hide">刘新冬的博客</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link">
  首页
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link">
  首页
</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><p>深度神经网络学到的所有变换都可以简化为数值数据张量上的一些 <strong>张量运算（tensor operation）</strong>，例如加上张量、乘以张量等。</p> <h2 id="_1-逐元素运算"><a href="#_1-逐元素运算" class="header-anchor">#</a> 1. 逐元素运算</h2> <p>Keras 中 relu运算和加法都是逐元素（element-wise）的运算，即该运算独立地应用于张量中的每个元素，也就是说，这些运算非常适合大规模并行实现（向量化实现）。</p> <p>如果你想对逐元素运算编写简单的 Python 实现，那么可以用 for 循环。下列代码是对逐元素 relu 运算的简单实现。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">naive_relu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>    <span class="token operator">//</span> x 是一个Numpy的2D张量
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token operator">//</span> 避免覆盖输入张量
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x
</code></pre></div><p>对于加法采用同样的实现方法。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">naive_add</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>        <span class="token operator">//</span> x 和 y 是Numpy的2D张量
    <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape <span class="token operator">==</span> y<span class="token punctuation">.</span>shape
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>                    <span class="token operator">//</span> 避免覆盖输入张量
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">+=</span> y<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span>
    <span class="token keyword">return</span> x
</code></pre></div><p>根据同样的方法，你可以实现逐元素的乘法、减法等。</p> <p>在实践中处理 Numpy 数组时，这些运算都是优化好的 Numpy 内置函数，这些函数将大量运算交给安装好的基础线性代数子程序（BLAS， basic linear algebra subprograms）实现（没装
的话，应该装一个）。 BLAS 是低层次的、高度并行的、高效的张量操作程序，通常用 Fortran 或 C 语言来实现。</p> <p>因此，在 Numpy 中可以直接进行下列逐元素运算，速度非常快。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
z <span class="token operator">=</span> x <span class="token operator">+</span> y                   <span class="token operator">//</span> 逐元素的相加
z <span class="token operator">=</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>z<span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">)</span>       <span class="token operator">//</span> 逐元素的relu
</code></pre></div><h2 id="_2-广播"><a href="#_2-广播" class="header-anchor">#</a> 2. 广播</h2> <p>如果将两个形状不同的张量相加，较小的张量会被广播（broadcast），以匹配较大张量的形状。</p> <p>广播包含以下两步。</p> <ol><li><p>向较小的张量添加轴（叫作<strong>广播轴</strong>），使其 ndim 与较大的张量相同。</p></li> <li><p>将较小的张量沿着新轴重复，使其形状与较大的张量相同。</p></li></ol> <p>来看一个具体的例子。假设 X 的形状是 (32, 10)， y 的形状是 (10,)。首先，我们给 y 添加空的第一个轴，这样 y 的形状变为 (1, 10)。然后，我们将 y 沿着新轴重复 32 次，这样
得到的张量 Y 的形状为 (32, 10)，并且 <code>Y[i, :] == y for i in range(0, 32)</code>。现在，我们可以将 X 和 Y 相加，因为它们的形状相同。</p> <p>在实际的实现过程中并不会创建新的 2D 张量，因为那样做非常低效。重复的操作完全是虚拟的，它只出现在算法中，而没有发生在内存中。但想象将向量沿着新轴重复 10 次，是一种很有用的思维模型。下面是一种简单的实现。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">naive_add_matrix_and_vector</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>            <span class="token operator">//</span> x 是一个 Numpy 的 2D 张量
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>            <span class="token operator">//</span> y 是一个 Numpy 向量
    <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>                        <span class="token operator">//</span> 避免覆盖输入张量
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">+=</span> y<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
    <span class="token keyword">return</span> x
</code></pre></div><p>如果一个张量的形状是 (a, b, ... n, n+1, ... m)，另一个张量的形状是 (n, n+1, ... m)，那么你通常可以利用广播对它们做两个张量之间的逐元素运算。广播操作会自动应用于从 a 到 n-1 的轴。</p> <p>下面这个例子利用广播将逐元素的 maximum 运算应用于两个形状不同的张量。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token operator">//</span> x 是形状为 <span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> 的随机张量
y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token operator">//</span> y 是形状为 <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> 的随机张量
z <span class="token operator">=</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>                    <span class="token operator">//</span> 输出 z 的形状是 <span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>，与 x 相同
</code></pre></div><h2 id="_3-点积"><a href="#_3-点积" class="header-anchor">#</a> 3. 点积</h2> <p>点积运算，也叫 <strong>张量积</strong>（tensor product，不要与逐元素的乘积弄混），是最常见也最有用的张量运算。与逐元素的运算不同，它将输入张量的元素合并在一起。</p> <p>在 Numpy、 Keras、 Theano 和 TensorFlow 中，都是用 * 实现逐元素乘积。 TensorFlow 中的点积使用了不同的语法，但在 Numpy 和 Keras 中，都是用标准的 dot 运算符来实现点积。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
z <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre></div><p>数学符号中的点（.）表示点积运算。</p> <div class="language-python extra-class"><pre class="language-python"><code>z<span class="token operator">=</span>x<span class="token punctuation">.</span>y
</code></pre></div><p>从数学的角度来看，点积运算做了什么？我们首先看一下两个向量 x 和 y 的点积。其计算过程如下。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">naive_vector_dot</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>            <span class="token operator">//</span> x 和 y 都是 Numpy 向量
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>
    <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    z <span class="token operator">=</span> <span class="token number">0.</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        z <span class="token operator">+=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
    <span class="token keyword">return</span> z
</code></pre></div><p>注意，两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。</p> <p>你还可以对一个矩阵 x 和一个向量 y 做点积，返回值是一个向量，其中每个元素是 y 和 x 的每一行之间的点积。其实现过程如下。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">def</span> <span class="token function">naive_matrix_vector_dot</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>                <span class="token operator">//</span> x 是一个 Numpy 矩阵
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>                <span class="token operator">//</span> y 是一个 Numpy 向量
    <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>         <span class="token operator">//</span> x 的第 <span class="token number">1</span> 维和 y 的第 <span class="token number">0</span> 维大小必须相同
    z <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token operator">//</span> 这个运算返回一个全是 <span class="token number">0</span> 的向量，其形状与 x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> 相同
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            z<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> y<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
    <span class="token keyword">return</span> z
</code></pre></div><p>你还可以复用前面写过的代码，从中可以看出矩阵 - 向量点积与向量点积之间的关系。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">naive_matrix_vector_dot</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    z <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        z<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> naive_vector_dot<span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> z
</code></pre></div><p>注意，如果两个张量中有一个的 ndim 大于 1，那么 dot 运算就不再是对称的，也就是说，dot(x, y) 不等于 dot(y, x)。</p> <p>当然，点积可以推广到具有任意个轴的张量。最常见的应用可能就是两个矩阵之间的点积。对于两个矩阵 x 和 y，当且仅当 x.shape[1] == y.shape[0] 时，你才可以对它们做点积（dot(x, y)）。得到的结果是一个形状为 (x.shape[0], y.shape[1]) 的矩阵，其元素为 x 的行与 y 的列之间的点积。其简单实现如下。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">naive_matrix_dot</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>                        <span class="token operator">//</span> x 和 y 都是Numpy矩阵
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>
    <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                 <span class="token operator">//</span> x 的第 <span class="token number">1</span> 维和 y 的第 <span class="token number">0</span> 维大小必须相同
    z <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token operator">//</span> 这个运算返回特定形状的零矩阵
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                     <span class="token operator">//</span> 遍历 x 的所有行
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                 <span class="token operator">//</span> 然后遍历 y 的所有列
            row_x <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
            column_y <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>
            z<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> naive_vector_dot<span class="token punctuation">(</span>row_x<span class="token punctuation">,</span> column_y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> z
</code></pre></div><p>为了便于理解点积的形状匹配，可以将输入张量和输出张量像图 2-5 中那样排列，利用可视化来帮助理解。</p> <img src="/img/deeplearning/2-5.png" style="zoom:100%;"> <p>图 2-5 中， x、 y 和 z 都用矩形表示（元素按矩形排列）。 x 的行和 y 的列必须大小相同，因此 x 的宽度一定等于 y 的高度。如果你打算开发新的机器学习算法，可能经常要画这种图。</p> <p>更一般地说，你可以对更高维的张量做点积，只要其形状匹配遵循与前面 2D 张量相同的原则：</p> <p>(a, b, c, d) . (d,) -&gt; (a, b, c)</p> <p>(a, b, c, d) . (d, e) -&gt; (a, b, c, e)</p> <p>以此类推。</p> <h2 id="_4-变形"><a href="#_4-变形" class="header-anchor">#</a> 4. 变形</h2> <p><strong>张量变形（tensor reshaping）</strong> 是指改变张量的行和列，以得到想要的形状。变形后的张量的元素总个数与初始张量相同。简单的例子可以帮助我们理解张量变形。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">5.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
       
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x
array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span> <span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><p>经常遇到的一种特殊的张量变形是 <strong>转置（transposition）</strong>。对矩阵做转置是指将行和列互换，使 x[i, :] 变为 x[:, i]。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x <span class="token operator">=</span> np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">)</span>
</code></pre></div><h2 id="_5-张量运算的几何解释"><a href="#_5-张量运算的几何解释" class="header-anchor">#</a> 5. 张量运算的几何解释</h2> <p>对于张量运算所操作的张量，其元素可以被解释为某种几何空间内点的坐标，因此所有的张量运算都有几何解释。举个例子，我们来看加法。首先有这样一个向量：</p> <p>A = [0.5, 1]</p> <p>它是二维空间中的一个点（见图 2-6）。常见的做法是将向量描绘成原点到这个点的箭头，如图 2-7 所示。</p> <img src="/img/deeplearning/2-6.png" style="zoom:100%;"> <p>假设又有一个点： B = [1, 0.25]，将它与前面的 A 相加。从几何上来看，这相当于将两个向量箭头连在一起，得到的位置表示两个向量之和对应的向量（见图 2-8）。</p> <img src="/img/deeplearning/2-8.png" style="zoom:100%;"> <p>通常来说，仿射变换、旋转、缩放等基本的几何操作都可以表示为张量运算。举个例子，要将一个二维向量旋转 theta 角，可以通过与一个 2 × 2 矩阵做点积来实现，这个矩阵为 R = [u, v]，其中 u 和 v 都是平面向量： u = [cos(theta), sin(theta)]， v = [-sin(theta), cos(theta)]。</p> <h2 id="_6-深度学习的几何解释"><a href="#_6-深度学习的几何解释" class="header-anchor">#</a> 6. 深度学习的几何解释</h2> <p>前面讲过，神经网络完全由一系列张量运算组成，而这些张量运算都只是输入数据的几何变换。因此，你可以将神经网络解释为高维空间中非常复杂的几何变换，这种变换可以通过许多简单的步骤来实现。</p> <p>对于三维的情况，下面这个思维图像是很有用的。想象有两张彩纸：一张红色，一张蓝色。将其中一张纸放在另一张上。现在将两张纸一起揉成小球。这个皱巴巴的纸球就是你的输入数据，每张纸对应于分类问题中的一个类别。神经网络（或者任何机器学习模型）要做的就是找到可以让纸球恢复平整的变换，从而能够再次让两个类别明确可分。通过深度学习，这一过程可以用三维空间中一系列简单的变换来实现，比如你用手指对纸球做的变换，每次做一个动作，如图 2-9 所示。</p> <img src="/img/deeplearning/2-9.png" style="zoom:100%;"> <p>让纸球恢复平整就是机器学习的内容：为复杂的、高度折叠的数据流形找到简洁的表示。现在你应该能够很好地理解，为什么深度学习特别擅长这一点：它将复杂的几何变换逐步分解为一长串基本的几何变换，这与人类展开纸球所采取的策略大致相同。深度网络的每一层都通过变换使数据解开一点点——许多层堆叠在一起，可以实现非常复杂的解开过程。</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"><!----></div></div>
    <script src="/blog/assets/js/app.ba74f136.js" defer></script><script src="/blog/assets/js/2.ba6b1436.js" defer></script><script src="/blog/assets/js/1.2d4ce02f.js" defer></script><script src="/blog/assets/js/32.e50cc8ec.js" defer></script>
  </body>
</html>
